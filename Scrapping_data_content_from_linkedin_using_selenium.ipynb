{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary required libraries\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Firefox(executable_path = \"/home/bluebrain/Documents/project_folder/Web_scrapper/geckodriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So basically what we have to do is change the link in the automated web browser after linkedin.com with /uas/login\n",
    "#We have to inspect the web page of login to see the id of the part which contain email and password\n",
    "#So on inspecting what we get is that id of email is username and that of password is password\n",
    "#Loading the web page of login\n",
    "browser.get(\"https://www.linkedin.com/uas/login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now opening the config.txt file and getting the email and password out of it into username and password\n",
    "f = open(\"config.txt\")\n",
    "lines = f.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logging in into the linkedin using automated software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now finding the element with id equal username in inspect element\n",
    "elementId = browser.find_element_by_id(\"username\")\n",
    "elementId.send_keys(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now finding the element with id equal password in inspect element\n",
    "elementId = browser.find_element_by_id(\"password\")\n",
    "elementId.send_keys(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summitting the keys\n",
    "elementId.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_id is basically the profile id which we want to visit for gatehering information of its connections or say his informatiom\n",
    "p_id = \"/in/sourabh-choudhary-849029179/\"\n",
    "#f_id is full profile id\n",
    "f_id = \"https://www.linkedin.com\"+p_id\n",
    "#using browser the get the profile web page\n",
    "browser.get(f_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This we are doing is because it is not initially loading the full profile(so we will be scrolling the profile till end)\n",
    "#scrolling the full web page\n",
    "scroll_pause_time = 5\n",
    "#getting the scroll height\n",
    "last_height = browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "for i in range(3):\n",
    "    #scroll down to bottom\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    #wait to load page\n",
    "    time.sleep(scroll_pause_time)\n",
    "    #calculating the new scroll height and comparing with previous scroll height\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting the page source(full page source)\n",
    "source = browser.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the source into beautifulsoup object and the parser is lxml\n",
    "soup = BeautifulSoup(source,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"flex-1 mr5\">\n",
       " <ul class=\"pv-top-card--list inline-flex align-items-center\">\n",
       " <li class=\"inline t-24 t-black t-normal break-words\">\n",
       "             Sourabh Choudhary\n",
       "           </li>\n",
       " <!-- --> <li class=\"inline-flex ml2\">\n",
       " <span class=\"pv-member-badge--for-top-card inline-flex pv-member-badge ember-view\" id=\"ember75\" style=\"display: none;\"><!-- -->\n",
       " <!-- -->\n",
       " <span class=\"visually-hidden\">\n",
       "   Sourabh has a  account\n",
       " </span>\n",
       " <!-- --></span>\n",
       " </li>\n",
       " </ul>\n",
       " <h2 class=\"mt1 t-18 t-black t-normal\">\n",
       "             Student at IIIT UNA\n",
       "           </h2>\n",
       " <ul class=\"pv-top-card--list pv-top-card--list-bullet mt1\">\n",
       " <li class=\"t-16 t-black t-normal inline-block\">\n",
       "               Muzaffarnagar, Uttar Pradesh, India\n",
       "             </li>\n",
       " <!-- -->\n",
       " <li class=\"inline-block\">\n",
       " <a class=\"ember-view\" data-control-name=\"topcard_view_all_connections\" href=\"/search/results/people/?facetNetwork=%5B%22F%22%5D&amp;origin=MEMBER_PROFILE_CANNED_SEARCH\" id=\"ember76\"> <span class=\"t-16 t-bold\">\n",
       "                     138 connections\n",
       "                   </span>\n",
       " </a> </li>\n",
       " <li class=\"inline-block\">\n",
       " <a class=\"ember-view\" data-control-name=\"contact_see_more\" href=\"/in/sourabh-choudhary-849029179/detail/contact-info/\" id=\"ember77\"> <span class=\"t-16 t-bold\">\n",
       "                 Contact info\n",
       "               </span>\n",
       " </a> </li>\n",
       " </ul>\n",
       " </div>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now basically inspecting the whole page(using inspect option) to get the specific data what we need from the web page\n",
    "#We will be looking for the div tag having class flex-1 mr5(as per our need)\n",
    "name_div = soup.find_all(\"div\",{\"class\":\"flex-1 mr5\"})\n",
    "name_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now basically we are getting all the unordered list\n",
    "name_location = name_div.find_all(\"ul\")\n",
    "#Taking the text content of the list\n",
    "#Basically this name contains the name of the person whose id we are scrapping\n",
    "name = name_location[0].find(\"li\").get_text().strip()\n",
    "location = name_location[1].find(\"li\").get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the profile title\n",
    "profile_title = name_div.find(\"h2\").get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the no of connections of a person\n",
    "connections = name_location.find_all(\"li\")\n",
    "connections = connections[1].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_information = []\n",
    "#appending all the information into the profile_information\n",
    "a = [name,location,profile_title,connections]\n",
    "for i in range(len(a)):\n",
    "    profile_information.append(a[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the way we scrap data from websites(some more clever techniques can also be applied like getting all the links of profile_ids and running a loop for all the profile ids and performing the same operations as we did on all of them to collect the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
