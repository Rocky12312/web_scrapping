{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using requests module to get data from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we basically use requests module to make a get HTTP requests for the url\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html\\nlang=en-US id=arve><head><meta\\ncharset=UTF-8><meta\\nhttp-equiv=X-UA-Compatible content=\"IE=10\"><link\\nrel=profile href=http://gmpg.org/xfn/11><link\\nrel=pingback href=https://authora'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rqst = requests.get(\"https://authoraditiagarwal.com\")\n",
    "rqst.text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib is another Python library that can be used for retrieving data from URLs similar to the requests library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bluebrain/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn and grow together\n"
     ]
    }
   ],
   "source": [
    "#Using urllib3 to get the raw html data from web page\n",
    "import urllib3\n",
    "#We will use BeautifulSoup to parse the html documents or say the data\n",
    "from bs4 import BeautifulSoup\n",
    "http = urllib3.PoolManager()\n",
    "rqst = http.request(\"GET\", \"https://authoraditiagarwal.com\")\n",
    "soup = BeautifulSoup(rqst.data, \"lxml\")\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using selenium to get the data from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "browser = webdriver.Firefox(executable_path = \"/home/bluebrain/Documents/project_folder/Web_scrapper/geckodriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now basically providing the url which we want to open in that web browser controlled by our python script\n",
    "browser.get('https://authoraditiagarwal.com/leadershipmanagement')\n",
    "#Scrape a particular element by providing the xpath as provided in lxml\n",
    "browser.find_element_by_xpath('/html/body').click()\n",
    "#A browser will pop up after executing the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researching prior to scrapping a website to check whether the website allow scrapping or not \n",
    "basically if we are using the data for a personal use it is okk but if we are going to publish\n",
    "the data we should first get the permission from the author of website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some ways by which we can check whether a website allow scrapping of its data or not and if \n",
    "it allows we can use any of the above module to scrap the data from website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robots.txt\n",
    "\n",
    "First file which we need to analyze before scrapping the website is robots.txt\n",
    "Robots.txt basically contain the information about which part of the website we can scrap and we can't(disallowed)\n",
    "We can check this file by putting a slash robots.txt after the url of website\n",
    "Here the publisher of website basically defines the rules over the part of the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sitemap file\n",
    "\n",
    "Basically when we need to crawl a website for getting the updated information what happens is basically that\n",
    "for getting only the updated information we will have to crawl over all the web pages but we can avoid that \n",
    "and use Sitemap file provided by the website to check for the updated information on a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "site.xyz.com\n",
    "\n",
    "We can analyze the size of website before crawling the website by typing site.xyz.com where xyz is the name of \n",
    "the website of which we are checking the size(the no of results which we will see will be the size of website).\n",
    "This might help because if a website is of much greater size it can take even months to crawl the website and \n",
    "also the efficiency will be poor in scrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it is also the technology used in making the website affects the way we crawl a website so it is better to\n",
    "check the technology used to make the website(@We can check it with python library builtwith)\n",
    "@We can use \"whois\" to also check the owner of website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'web-servers': ['Apache'],\n",
       " 'javascript-graphics': ['D3'],\n",
       " 'advertising-networks': ['Google AdSense'],\n",
       " 'cache-tools': ['W3 Total Cache'],\n",
       " 'cms': ['WordPress'],\n",
       " 'programming-languages': ['PHP'],\n",
       " 'blogs': ['PHP', 'WordPress']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The technology made in using the site\n",
    "import builtwith\n",
    "builtwith.parse(\"https://authoraditiagarwal.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now analyzing the web page(this is the most importtant step in scrapping because without knowing the structure we will not be able to know in which form we will receive the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze a webpage by viewing the page source(or by inspecting the page source by clicking the inspect \n",
    "element option).Basically there we will get the data of our interest from that web page but in the form of HTML\n",
    "and the main concern will be about whitespaces and formatting which is difficult for us to format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup is a Python library for pulling data out of HTML and XML files. It can be used with \n",
    "requests, because it needs an input (document or url) to create a soup object as it cannot fetch a \n",
    "web page by itself(also it can be used to get specific data parts of websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "page_url =  \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "#We now make the connection to the webpage and we can parse the html using BeautifulSoup, storing the object in the variable ‘soup’\n",
    "#Returning the url page\n",
    "page = urllib.request.urlopen(page_url)\n",
    "#Parsing the html using BeautifulSoup and storing in variable soup\n",
    "#lxml is basically the tree structure we want\n",
    "soup = BeautifulSoup(page,\"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the find_all() method of soup to extract useful html tags within a webpage. Examples of useful tags include < a > for hyperlinks, < table > for tables, < tr > for table rows, < th > for table headers, and < td > for table cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Artificial intelligence - Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "#getting the title of the web page\n",
    "title = soup.title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the text of the web page\n",
    "text = soup.get_text()\n",
    "#print(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the links from the web page\n",
    "All_links = soup.find_all(\"a\")\n",
    "a = []\n",
    "for link in All_links:\n",
    "    a.append(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5133\n"
     ]
    }
   ],
   "source": [
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing out table rows\n",
    "rows = soup.find_all(\"tr\")\n",
    "#print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2 id=\"mw-toc-heading\">Contents</h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"History\">History</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=1\" title=\"Edit section: History\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Definitions\">Definitions</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=2\" title=\"Edit section: Definitions\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Basics\">Basics</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=3\" title=\"Edit section: Basics\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Challenges\">Challenges</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=4\" title=\"Edit section: Challenges\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Approaches\">Approaches</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=14\" title=\"Edit section: Approaches\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Tools\">Tools</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=26\" title=\"Edit section: Tools\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Applications\">Applications<span id=\"Goals\"></span></span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=35\" title=\"Edit section: Applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Philosophy_and_ethics\">Philosophy and ethics</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=48\" title=\"Edit section: Philosophy and ethics\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Economics\">Economics</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=68\" title=\"Edit section: Economics\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Regulation\">Regulation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=69\" title=\"Edit section: Regulation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"In_fiction\">In fiction</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=70\" title=\"Edit section: In fiction\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=71\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Explanatory_notes\">Explanatory notes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=72\" title=\"Edit section: Explanatory notes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=73\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"Further_reading\">Further reading</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=77\" title=\"Edit section: Further reading\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Artificial_intelligence&amp;action=edit&amp;section=78\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>,\n",
       " <h2>Navigation menu</h2>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use the select method on soup to get whatever we want from our parsed page source(Really useful)\n",
    "#If we have a class we have to basically use a dot and then a classname(.classname) to select the data\n",
    "#Basically analyzing this approach is really useful\n",
    "soup.select(\"h2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all the text from the web page(similarly we can get links etc)and storing it in a text string\n",
    "text_string = \"\"\n",
    "#Using the paragraph tag\n",
    "for string in soup.find_all(\"p\"):\n",
    "    text_string = text_string+string.text\n",
    "#print(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now preprocessing the data\n",
    "#lowering the text,removing digits etc\n",
    "import re\n",
    "#Removing all the numbers from the text\n",
    "text_string = re.sub(r\"\\[[0-9]*\\]\",\" \",text_string)\n",
    "#Removing the spaces created by replacing the numbers with spaces\n",
    "text_string = re.sub(r\"\\s+\",\" \",text_string)\n",
    "#Lowering the text\n",
    "text_string=text_string.lower()\n",
    "text_string = re.sub(r\"\\d\",\" \",text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bluebrain/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bluebrain/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "#Sentence tokenizing\n",
    "sentences = nltk.sent_tokenize(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word tokenizing the sentences\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically how we actually process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
